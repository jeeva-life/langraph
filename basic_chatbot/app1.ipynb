{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "    else:\n",
    "        print( os.environ.get(var))\n",
    "\n",
    "_set_env(\"ANTHROPIC_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1ee45293fe0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            \n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstream_graph_updates\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhi, how r u\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m, in \u001b[0;36mstream_graph_updates\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstream_graph_updates\u001b[39m(user_input: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgraph\u001b[49m\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input)]}):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m      4\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m, value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "stream_graph_updates(\"hi, how r u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.tools import Tool\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_neo4j import Neo4jChatMessageHistory, Neo4jGraph\n",
    "from uuid import uuid4\n",
    "\n",
    "SESSION_ID = str(uuid4())\n",
    "print(f\"Session ID: {SESSION_ID}\")\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=\"sk-...\")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"pleaseletmein\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a movie expert. You find movies from a genre or plot.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "movie_chat = prompt | llm | StrOutputParser()\n",
    "\n",
    "def get_memory(session_id):\n",
    "    return Neo4jChatMessageHistory(session_id=session_id, graph=graph)\n",
    "\n",
    "#A tool is created using the chain:, In this example, the Tool is created from a function. The function is the movie_chat.invoke method.\n",
    "# The name and description help the LLM select which tool to use when presented with a question. The func parameter is the function that \n",
    "# will be called when the tool is selected. The return_direct flag indicates that the tool will return the result directly.\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Chat\",\n",
    "        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n",
    "        func=movie_chat.invoke,\n",
    "    )\n",
    "]\n",
    "\n",
    "#An agent is created that uses the tool:\n",
    "#An agent requires a prompt. You could create a prompt, but in this example, the program pulls\n",
    "#  a pre-existing prompt from the Langsmith Hub.\n",
    "# The hwcase17/react-chat prompt instructs the model to provide an answer using the tools available in a specific format.\n",
    "agent_prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "agent = create_react_agent(llm, tools, agent_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "#The AgentExecutor class runs the agent. It expects the following parameters:\n",
    "    '''The agent to run:\n",
    "    The tools that the agent can use:\n",
    "    The memory which will store the conversation history'''\n",
    "\n",
    "#The agent is wrapped in a RunnableWithMessageHistory chain that allows it to interact with the memory:\n",
    "chat_agent = RunnableWithMessageHistory(\n",
    "    agent_executor,\n",
    "    get_memory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "while True:\n",
    "    q = input(\"> \")\n",
    "\n",
    "    response = chat_agent.invoke(\n",
    "        {\n",
    "            \"input\": q\n",
    "        },\n",
    "        {\"configurable\": {\"session_id\": SESSION_ID}},\n",
    "    )\n",
    "    \n",
    "    print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Chat\",\n",
    "        description=\"For when you need to chat about movies. The question will be a string. Return a string.\",\n",
    "        func=movie_chat.invoke,\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        name=\"Movie Trailer Search\",\n",
    "        description=\"Use when needing to find a movie trailer. The question will include the word trailer. Return a link to a YouTube video.\",\n",
    "        func=call_trailer_search,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Querying a vector index\\nReview the following code that creates a Neo4jVector from the moviePlots index you created.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Querying a vector index\n",
    "Review the following code that creates a Neo4jVector from the moviePlots index you created.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
    "\n",
    "embedding_provider = OpenAIEmbeddings(\n",
    "    openai_api_key=\"sk-...\"\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"pleaseletmein\"\n",
    ")\n",
    "\n",
    "movie_plot_vector = Neo4jVector.from_existing_index(\n",
    "    embedding_provider,\n",
    "    graph=graph,\n",
    "    index_name=\"moviePlots\",\n",
    "    embedding_node_property=\"plotEmbedding\",\n",
    "    text_node_property=\"plot\",\n",
    ")\n",
    "\n",
    "result = movie_plot_vector.similarity_search(\"A movie where aliens land and attack earth.\")\n",
    "for doc in result:\n",
    "    print(doc.metadata[\"title\"], \"-\", doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new vector index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new vector index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code would create embeddings and a new index called myVectorIndex in the database for Chunk nodes with a text property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_neo4j\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Neo4jGraph, Neo4jVector\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_openai'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
    "from langchain.schema import Document\n",
    "\n",
    "# A list of Documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Text to be indexed\",\n",
    "        metadata={\"source\": \"local\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Service used to create the embeddings\n",
    "embedding_provider = OpenAIEmbeddings(\n",
    "    openai_api_key=\"sk-...\"\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"pleaseletmein\"\n",
    ")\n",
    "\n",
    "new_vector = Neo4jVector.from_documents(\n",
    "    documents,\n",
    "    embedding_provider,\n",
    "    graph=graph,\n",
    "    index_name=\"myVectorIndex\",\n",
    "    node_label=\"Chunk\",\n",
    "    text_node_property=\"text\",\n",
    "    embedding_node_property=\"embedding\",\n",
    "    create_id_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Retriever chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By incorporating Neo4jVector into a RetrievalQA chain, you can use data and vectors in Neo4j in a Langchain application.\n",
    "\n",
    "Review this program incorporating the moviePlots vector index into a retrieval chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jGraph, Neo4jVector\n",
    "\n",
    "OPENAI_API_KEY = \"sk-...\"\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "embedding_provider = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"pleaseletmein\"\n",
    ")\n",
    "\n",
    "movie_plot_vector = Neo4jVector.from_existing_index(\n",
    "    embedding_provider,\n",
    "    graph=graph,\n",
    "    index_name=\"moviePlots\",\n",
    "    embedding_node_property=\"plotEmbedding\",\n",
    "    text_node_property=\"plot\",\n",
    ")\n",
    "\n",
    "plot_retriever = RetrievalQA.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=movie_plot_vector.as_retriever()\n",
    ")\n",
    "\n",
    "response = plot_retriever.invoke(\n",
    "    {\"query\": \"A movie where a mission to the moon goes wrong\"}\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Cypher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain includes the GraphCypherQAChainchain that can interact with a Neo4j graph database. It uses a language model to generate Cypher queries and then uses the graph to answer the question.\n",
    "The program below will generate a Cypher query based on the schema in the graph database and the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_neo4j import GraphCypherQAChain, Neo4jGraph\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=\"sk-...\"\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"pleaseletmein\",\n",
    ")\n",
    "\n",
    "CYPHER_GENERATION_TEMPLATE = \"\"\"\n",
    "You are an expert Neo4j Developer translating user questions into Cypher to answer questions about movies and provide recommendations.\n",
    "Convert the user's question based on the schema.\n",
    "\n",
    "Schema: {schema}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "cypher_generation_prompt = PromptTemplate(\n",
    "    template=CYPHER_GENERATION_TEMPLATE,\n",
    "    input_variables=[\"schema\", \"question\"],    #The schema will be automatically generated from the graph database and passed to the LLM. The question will be the user’s question.\n",
    ")\n",
    "\n",
    "cypher_chain = GraphCypherQAChain.from_llm(\n",
    "    llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=cypher_generation_prompt,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "cypher_chain.invoke({\"query\": \"What is the plot of the movie Toy Story?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
